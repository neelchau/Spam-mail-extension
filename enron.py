# -*- coding: utf-8 -*-
"""enron.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1QvTli0nzLav3IOmH2KHCKtxsQa6ogUj5

# Importing Libraries and Data
"""

import pandas as pd
import string
import re
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.naive_bayes import MultinomialNB
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score, classification_report
from scipy.sparse import hstack
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords
import nltk
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')
nltk.download('punkt_tab')
import warnings
warnings.filterwarnings('ignore')

data = pd.read_csv('drive/MyDrive/enron_spam_data.csv')

data

"""# Handling Missing Values and Dates"""

data = data.dropna(subset=['Message'])
data

data.isna().sum()

data['Date'] = pd.to_datetime(data['Date'], errors='coerce')

data

data.drop(columns=['Message ID'], inplace=True)
data

"""# ads"""

"""# Data Preprocessing"""

lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    text = text.lower()  # Convert to lowercase
    text = re.sub(r'http\S+|www\S+', 'URL', text)  # Replace URLs
    text = re.sub(r'\S+@\S+', 'EMAIL', text)  # Replace email addresses
    text = re.sub(r'[^a-zA-Z0-9\s]', '', text)  # Remove special characters
    text = re.sub(r'\s+', ' ', text).strip()  # Remove extra whitespace
    text = re.sub(r'(.)\1{2,}', r'\1', text)  # Normalize repeated characters
    words = word_tokenize(text)  # Tokenization
    words = [lemmatizer.lemmatize(word) for word in words if word not in stop_words]  # Lemmatization & Stopword removal
    return ' '.join(words)

data['Subject'] = data['Subject'].astype(str)
data['Message'] = data['Message'].astype(str)

data['Processed_Subject'] = data['Subject'].apply(preprocess_text)
data['Processed_Message'] = data['Message'].apply(preprocess_text)

"""# Feature Enginnering"""

data["Spam/Ham"]=data['Spam/Ham'].replace({'spam':1,'ham':0})
data

def count_digits(text):
    return sum(1 for char in text if char.isdigit())

data['Subject_Length'] = data['Subject'].astype(str).apply(len)
data['Message_Length'] = data['Message'].astype(str).apply(len)
data['Subject_Word_Count'] = data['Subject'].astype(str).apply(lambda x: len(x.split()))
data['Message_Word_Count'] = data['Message'].astype(str).apply(lambda x: len(x.split()))
data['Subject_Digits'] = data['Subject'].astype(str).apply(count_digits)
data['Message_Digits'] = data['Message'].astype(str).apply(count_digits)

data

data.to_csv('spam5.csv',index=False)

"""# TF-IDF VECTORIZATION"""

# data = pd.read_csv('/content/drive/MyDrive/spam5.csv')

vectorizer_subject = TfidfVectorizer(stop_words='english', max_features=2000, ngram_range=(1,2))
vectorizer_message = TfidfVectorizer(stop_words='english', max_features=5000, ngram_range=(1,2))

X_subject = vectorizer_subject.fit_transform(data['Processed_Subject'])
X_message = vectorizer_message.fit_transform(data['Processed_Message'])

"""# Normalization"""

numeric_features = data[['Subject_Length', 'Message_Length', 'Subject_Word_Count', 'Message_Word_Count',
                         'Subject_Digits', 'Message_Digits']]

scaler = MinMaxScaler()
X_numeric = scaler.fit_transform(numeric_features)

"""# Data Split and Training"""

X = hstack([X_subject, X_message, X_numeric])
y = data['Spam/Ham']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

models = {
    "Logistic Regression": LogisticRegression(),
    "Naive Bayes": MultinomialNB(),
    "Random Forest": RandomForestClassifier(),
    "SVM": SVC(),
    "Decision Tree": DecisionTreeClassifier(),
    "K-Nearest Neighbors": KNeighborsClassifier()
}

for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    print(f"\n{name} Performance:")
    print("Accuracy:", accuracy_score(y_test, y_pred))
    print(classification_report(y_test, y_pred))

from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier

# Boosting Models
models = {
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
    "LightGBM": LGBMClassifier(),
    "AdaBoost": AdaBoostClassifier(n_estimators=100),
    "Gradient Boosting": GradientBoostingClassifier()
}

# Train & Evaluate Models
results = {}
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    acc = accuracy_score(y_test, y_pred)
    results[name] = acc
    print(f"\n{name} Performance:")
    print("Accuracy:", acc)
    print(classification_report(y_test, y_pred))

# Compare Performance
best_model = max(results, key=results.get)
print("\nðŸ“Š Performance Comparison:")
for model, acc in results.items():
    print(f"{model}: {acc:.4f}")

print(f"\nðŸš€ Best Performing Model: {best_model} with {results[best_model]:.4f} Accuracy")

from sklearn.ensemble import StackingClassifier

base_learners = [
    ('rf', RandomForestClassifier(n_estimators=100)),
    ('lgbm', LGBMClassifier()),
    ('lr', LogisticRegression())
]

# Define the meta-learner (Logistic Regression)
meta_learner = LogisticRegression()

# Create the Stacking model
stacking_model = StackingClassifier(estimators=base_learners, final_estimator=meta_learner)

# Train the stacking model
stacking_model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = stacking_model.predict(X_test)

# Evaluate the performance
accuracy = accuracy_score(y_test, y_pred)
print(f"Stacking Model Accuracy: {accuracy * 100:.2f}%")

"""# df"""

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.naive_bayes import MultinomialNB
from sklearn.svm import SVC
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score

# List of training dataset filenames
datasets = ["spam1.csv", "spam2.csv", "spam3.csv", "spam4.csv"]

# Load the test dataset
test_df = pd.read_csv("drive/MyDrive/test.csv")
X_test = test_df["message"].fillna("")  # Handle missing values in test data
y_test = LabelEncoder().fit_transform(test_df["label"].fillna("ham"))  # Encode labels

# TF-IDF Vectorizer (trained only on the training dataset vocabulary)
tfidf = TfidfVectorizer(stop_words="english", max_features=5000)

# Define models
models = {
    "NaÃ¯ve Bayes": MultinomialNB(),
    "SVM": SVC(),
    "Random Forest": RandomForestClassifier(),
    "Logistic Regression": LogisticRegression(),
    "Decision Tree": DecisionTreeClassifier(),
    "K-Nearest Neighbors": KNeighborsClassifier(),
}

# Initialize results dictionary
results = {model: [] for model in models.keys()}

for dataset in datasets:
    # Load training dataset
    train_df = pd.read_csv(f'/content/drive/MyDrive/{dataset}')

    # Handle missing values
    X_train = train_df["email"].fillna("")
    y_train = LabelEncoder().fit_transform(train_df["label"].fillna("ham"))

    # Fit TF-IDF on training data and transform both training & test data
    X_train_tfidf = tfidf.fit_transform(X_train)
    X_test_tfidf = tfidf.transform(X_test)

    # Train and evaluate each model
    for model_name, model in models.items():
        model.fit(X_train_tfidf, y_train)
        y_pred = model.predict(X_test_tfidf)
        acc = accuracy_score(y_test, y_pred)
        results[model_name].append(acc)

# Create DataFrame from results
accuracy_table = pd.DataFrame(results, index=datasets)

# Display results
print(accuracy_table.T)  # Transpose for better readability